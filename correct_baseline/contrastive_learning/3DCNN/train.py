import os
import numpy as np
import tifffile
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
import random
import time
from tqdm import tqdm
import json
import gc

# ====================== æ•°æ®é›†ç±» (å·²ä¿®æ”¹) ======================
class NeuronConnectivityDataset(Dataset):
    """ç¥ç»å…ƒè¿æ¥æ•°æ®é›†ç±»ï¼Œæ”¯æŒå¹³ç§»æ•°æ®å¢å¼º"""
    def __init__(self, data_root: str, img_dir: str, seg_dir: str, 
                 split_file: str, volume_size: tuple = (128, 128, 128),
                 augment: bool = False, max_translation: int = 10): # æ–°å¢ augment å’Œ max_translation å‚æ•°
        """
        åˆå§‹åŒ–æ•°æ®é›†
        data_root: æ•°æ®æ ¹ç›®å½•
        img_dir: å›¾åƒä½“ç§¯ç›®å½•å
        seg_dir: åˆ†å‰²æ©ç ç›®å½•å
        split_file: åˆ’åˆ†æ–‡ä»¶è·¯å¾„
        volume_size: ä½“ç§¯å°ºå¯¸ (D, H, W)
        augment: æ˜¯å¦å¯ç”¨æ•°æ®å¢å¼º (å¹³ç§»)
        max_translation: æœ€å¤§å¹³ç§»åƒç´ å€¼ (åœ¨æ¯ä¸ªç»´åº¦ä¸Šï¼Œæ­£è´Ÿ max_translation ä¹‹é—´)
        """
        self.data_root = data_root
        self.img_dir = os.path.join(data_root, img_dir)
        self.seg_dir = os.path.join(data_root, seg_dir)
        self.volume_size = volume_size
        self.augment = augment
        self.max_translation = max_translation
        
        # è¯»å–åˆ’åˆ†æ–‡ä»¶
        with open(os.path.join(data_root, split_file), 'r') as f:
            self.volume_list = [line.strip() for line in f]
        
        print(f"æ•°æ®é›†åˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {len(self.volume_list)} ä¸ªä½“ç§¯")
        if self.augment:
            print(f"æ•°æ®å¢å¼ºå·²å¯ç”¨: æœ€å¤§å¹³ç§» {self.max_translation} åƒç´ ")
    
    def __len__(self) -> int:
        """æ¯ä¸ªä½“ç§¯ç”Ÿæˆ1ä¸ªæ ·æœ¬ (åŒ…å«æ­£è´Ÿå¯¹ä¿¡æ¯)"""
        return len(self.volume_list)
    
    def __getitem__(self, idx: int) -> tuple:
        """è·å–æ ·æœ¬ï¼šåŸå§‹å›¾åƒ + å„è‡ªçš„æ©ç  + æ ‡ç­¾"""
        vol_name = self.volume_list[idx]
        
        # åŠ è½½å›¾åƒå’Œåˆ†å‰²
        img = tifffile.imread(os.path.join(self.img_dir, vol_name)).astype(np.float32)
        seg = tifffile.imread(os.path.join(self.seg_dir, vol_name))
        
        # ç¡®ä¿ä½“ç§¯å°ºå¯¸æ­£ç¡®
        if img.shape != self.volume_size:
            raise ValueError(f"ä½“ç§¯å°ºå¯¸ä¸åŒ¹é…: {img.shape} != {self.volume_size}")
        
        # ==================== åº”ç”¨æ•°æ®å¢å¼º (å¹³ç§») ====================
        if self.augment:
            D, H, W = self.volume_size
            
            # éšæœºç”Ÿæˆå¹³ç§»é‡
            # np.random.randint(low, high+1) ç”Ÿæˆ [low, high] èŒƒå›´å†…çš„æ•´æ•°
            shift_d = np.random.randint(-self.max_translation, self.max_translation + 1)
            shift_h = np.random.randint(-self.max_translation, self.max_translation + 1)
            shift_w = np.random.randint(-self.max_translation, self.max_translation + 1)

            # ä¸ºäº†è¿›è¡Œå¹³ç§»è£å‰ªï¼Œæˆ‘ä»¬å…ˆå¯¹å›¾åƒå’Œåˆ†å‰²æ©ç è¿›è¡Œå¡«å……
            # å¡«å……å¤§å°ä¸º max_translation
            padded_shape = (D + 2 * self.max_translation,
                            H + 2 * self.max_translation,
                            W + 2 * self.max_translation)
            
            padded_img = np.zeros(padded_shape, dtype=img.dtype)
            padded_seg = np.zeros(padded_shape, dtype=seg.dtype)

            # å°†åŸå§‹æ•°æ®æ”¾ç½®åœ¨å¡«å……åçš„ä¸­å¿ƒåŒºåŸŸ
            padded_img[self.max_translation : self.max_translation + D,
                       self.max_translation : self.max_translation + H,
                       self.max_translation : self.max_translation + W] = img
            
            padded_seg[self.max_translation : self.max_translation + D,
                       self.max_translation : self.max_translation + H,
                       self.max_translation : self.max_translation + W] = seg

            # è®¡ç®—è£å‰ªçš„èµ·å§‹åæ ‡
            # è¿™ä¸ªèµ·å§‹åæ ‡åŠ ä¸Š volume_size åï¼Œå°†ä»å¡«å……åçš„ä½“ç§¯ä¸­è£å‰ªå‡ºåŸå§‹å¤§å°çš„ä½“ç§¯ï¼Œ
            # ä¸”è¿™ä¸ªè£å‰ªåŒºåŸŸä¼šæ ¹æ® shift_d/h/w è¿›è¡Œåç§»ã€‚
            start_d = self.max_translation + shift_d
            start_h = self.max_translation + shift_h
            start_w = self.max_translation + shift_w

            # æ‰§è¡Œè£å‰ªæ“ä½œï¼Œç¡®ä¿å›¾åƒå’Œåˆ†å‰²æ©ç åº”ç”¨ç›¸åŒçš„å¹³ç§»
            img = padded_img[start_d : start_d + D,
                             start_h : start_h + H,
                             start_w : start_w + W]
            
            seg = padded_seg[start_d : start_d + D,
                             start_h : start_h + H,
                             start_w : start_w + W]
        # ==========================================================
        
        # æ ¹æ®è®ºæ–‡çš„ç®€åŒ–ï¼Œè¿™é‡Œç¡¬ç¼–ç äº†ç”¨äºæ­£è´Ÿæ ·æœ¬çš„ç‰‡æ®µID
        # å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›IDä¼šæ ¹æ®è¿æ¥æ³¨é‡ŠåŠ¨æ€ç”Ÿæˆ
        seg_id_query = 1
        seg_id_pos = 2
        seg_id_neg = 3

        # è·å–å„ä¸ªç‰‡æ®µçš„äºŒå€¼æ©ç  (è¿™é‡Œè·å–çš„æ©ç æ˜¯ç»è¿‡å¹³ç§»å¢å¼ºåçš„segä¸­æå–çš„)
        mask_query = self._get_mask(seg, seg_id_query) # M_A
        mask_pos = self._get_mask(seg, seg_id_pos)     # M_B (positive for M_A)
        mask_neg = self._get_mask(seg, seg_id_neg)     # M_C (negative for M_A)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        img_tensor = torch.from_numpy(img).unsqueeze(0) # [1, D, H, W]
        mask_query_tensor = torch.from_numpy(mask_query).unsqueeze(0) # [1, D, H, W]
        mask_pos_tensor = torch.from_numpy(mask_pos).unsqueeze(0)     # [1, D, H, W]
        mask_neg_tensor = torch.from_numpy(mask_neg).unsqueeze(0)     # [1, D, H, W]

        # æ­£è´Ÿæ ·æœ¬çš„æ ‡ç­¾ï¼Œè¿™é‡Œè¡¨ç¤º (query, pos) æ˜¯æ­£å¯¹ï¼Œ(query, neg) æ˜¯è´Ÿå¯¹
        label_pos_pair = torch.tensor(1.0) # Query-Pos Pair is Positive
        label_neg_pair = torch.tensor(0.0) # Query-Neg Pair is Negative
        
        # è¿”å›åŸå§‹å›¾åƒã€å„ä¸ªæ©ç ä»¥åŠå¯¹åº”çš„æ ‡ç­¾
        return img_tensor, mask_query_tensor, mask_pos_tensor, mask_neg_tensor, label_pos_pair, label_neg_pair
    
    def _get_mask(self, seg: np.ndarray, mask_id: int) -> np.ndarray:
        """æ ¹æ®mask_idæå–äºŒå€¼æ©ç """
        mask = np.zeros_like(seg, dtype=np.float32)
        mask[seg == mask_id] = 1
        return mask

# ====================== Squeeze-and-Excitation (SE) Block (ä¿æŒä¸å˜) ======================
class SEBlock(nn.Module):
    def __init__(self, channel, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1, 1)
        return x * y.expand_as(x)

# ====================== U-Net Conv Block (Encoder & Decoder) (ä¿æŒä¸å˜) ======================
class ConvBlock3D(nn.Module):
    """
    3Då·ç§¯å—ï¼ŒåŒ…å«ä¸¤ä¸ªConv3d -> BatchNorm3d -> ReLUï¼Œå¹¶å¯é€‰æ‹©æ€§åœ°åŒ…å«SEBlock
    """
    def __init__(self, in_channels, out_channels, use_se=True):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm3d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm3d(out_channels),
            nn.ReLU(inplace=True)
        )
        self.se_block = SEBlock(out_channels) if use_se else nn.Identity()

    def forward(self, x):
        x = self.block(x)
        x = self.se_block(x)
        return x

# ====================== EmbedNet (U-Net Architecture with SE) (ä¿æŒä¸å˜) ======================
class EmbedNetUNet(nn.Module):
    """
    åŸºäºU-Netæ¶æ„çš„EmbedNetï¼ŒåŒ…å«Squeeze-and-Excitationå±‚ã€‚
    è¾“å‡ºä½“ç´ çº§åµŒå…¥ã€‚
    """
    def __init__(self, in_channels=1, embed_dim=16, base_channels=32, use_se=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.in_channels = in_channels
        self.base_channels = base_channels
        
        # ç¼–ç å™¨ (ä¸‹é‡‡æ ·è·¯å¾„)
        self.enc1 = ConvBlock3D(in_channels, base_channels, use_se) # (128,128,128) -> (128,128,128)
        self.pool1 = nn.MaxPool3d(2) # (128,128,128) -> (64,64,64)

        self.enc2 = ConvBlock3D(base_channels, base_channels * 2, use_se) # (64,64,64) -> (64,64,64)
        self.pool2 = nn.MaxPool3d(2) # (64,64,64) -> (32,32,32)

        self.enc3 = ConvBlock3D(base_channels * 2, base_channels * 4, use_se) # (32,32,32) -> (32,32,32)
        self.pool3 = nn.MaxPool3d(2) # (32,32,32) -> (16,16,16)

        self.enc4 = ConvBlock3D(base_channels * 4, base_channels * 8, use_se) # (16,16,16) -> (16,16,16)
        self.pool4 = nn.MaxPool3d(2) # (16,16,16) -> (8,8,8)

        # åº•éƒ¨ (Bottleneck)
        self.bottleneck = ConvBlock3D(base_channels * 8, base_channels * 16, use_se) # (8,8,8) -> (8,8,8)

        # è§£ç å™¨ (ä¸Šé‡‡æ ·è·¯å¾„)
        self.upconv4 = nn.ConvTranspose3d(base_channels * 16, base_channels * 8, kernel_size=2, stride=2)
        self.dec4 = ConvBlock3D(base_channels * 16, base_channels * 8, use_se) # (base_channels*8 from upconv + base_channels*8 from enc4)

        self.upconv3 = nn.ConvTranspose3d(base_channels * 8, base_channels * 4, kernel_size=2, stride=2)
        self.dec3 = ConvBlock3D(base_channels * 8, base_channels * 4, use_se)

        self.upconv2 = nn.ConvTranspose3d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)
        self.dec2 = ConvBlock3D(base_channels * 4, base_channels * 2, use_se)

        self.upconv1 = nn.ConvTranspose3d(base_channels * 2, base_channels, kernel_size=2, stride=2)
        self.dec1 = ConvBlock3D(base_channels * 2, base_channels, use_se)

        # è¾“å‡ºå±‚ï¼Œå°†é€šé“æ•°æ˜ å°„åˆ° embed_dim
        self.out_conv = nn.Conv3d(base_channels, embed_dim, kernel_size=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ç¼–ç å™¨
        e1 = self.enc1(x)
        p1 = self.pool1(e1) # 64

        e2 = self.enc2(p1)
        p2 = self.pool2(e2) # 32

        e3 = self.enc3(p2)
        p3 = self.pool3(e3) # 16

        e4 = self.enc4(p3)
        p4 = self.pool4(e4) # 8

        # åº•éƒ¨
        b = self.bottleneck(p4) # 8

        # è§£ç å™¨
        d4 = self.upconv4(b) # 16
        # å¦‚æœå°ºå¯¸ä¸å®Œå…¨åŒ¹é…ï¼Œè¿›è¡Œè£å‰ª
        d4 = self._center_crop_and_concat(e4, d4) # Concat e4 (16) with d4 (16)
        d4 = self.dec4(d4)

        d3 = self.upconv3(d4) # 32
        d3 = self._center_crop_and_concat(e3, d3)
        d3 = self.dec3(d3)

        d2 = self.upconv2(d3) # 64
        d2 = self._center_crop_and_concat(e2, d2)
        d2 = self.dec2(d2)

        d1 = self.upconv1(d2) # 128
        d1 = self._center_crop_and_concat(e1, d1)
        d1 = self.dec1(d1)

        # è¾“å‡ºä½“ç´ çº§åµŒå…¥
        return self.out_conv(d1)

    def _center_crop_and_concat(self, enc_feature, dec_feature):
        """
        å¯¹ç¼–ç å™¨ç‰¹å¾å›¾è¿›è¡Œä¸­å¿ƒè£å‰ªï¼Œä»¥åŒ¹é…è§£ç å™¨ç‰¹å¾å›¾çš„å°ºå¯¸ï¼Œç„¶åæ‹¼æ¥ã€‚
        """
        enc_shape = enc_feature.shape[2:]
        dec_shape = dec_feature.shape[2:]

        diff_d = enc_shape[0] - dec_shape[0]
        diff_h = enc_shape[1] - dec_shape[1]
        diff_w = enc_shape[2] - dec_shape[2]

        if diff_d < 0 or diff_h < 0 or diff_w < 0:
            # Should not happen in typical U-Net, but for robustness
            raise ValueError("Decoder feature map is larger than encoder feature map for concatenation.")

        crop_d = diff_d // 2
        crop_h = diff_h // 2
        crop_w = diff_w // 2

        enc_feature_cropped = enc_feature[:, :,
                                          crop_d : enc_shape[0] - diff_d + crop_d,
                                          crop_h : enc_shape[1] - diff_h + crop_h,
                                          crop_w : enc_shape[2] - diff_w + crop_w]
        
        # æ‹¼æ¥è·³è·ƒè¿æ¥å’Œä¸Šé‡‡æ ·ç‰¹å¾
        return torch.cat([enc_feature_cropped, dec_feature], dim=1)


# ====================== å¯¹æ¯”æŸå¤±å‡½æ•° (ä¿æŒä¸å˜ï¼Œå·²æ ¹æ®å‰ä¸€æ¬¡å¯¹è¯ä¿®æ”¹) ======================
class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”æŸå¤±å‡½æ•° - å¤„ç†æ­£è´Ÿæ ·æœ¬å¯¹"""
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin
        self.distance = nn.PairwiseDistance(p=2) # æ¬§æ°è·ç¦»

    def forward(self, emb1: torch.Tensor, emb2: torch.Tensor, label: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å¯¹æ¯”æŸå¤±
        emb1, emb2: ä¸¤ä¸ªåµŒå…¥å‘é‡ (åˆ†æ®µå¹³å‡åµŒå…¥), [B, embed_dim]
        label: æ ‡ç­¾ (1è¡¨ç¤ºç›¸ä¼¼ï¼Œ0è¡¨ç¤ºä¸ç›¸ä¼¼), [B]
        """
        distance = self.distance(emb1, emb2)

        # å…¸å‹çš„å¯¹æ¯”æŸå¤±å½¢å¼ï¼š
        # L = y * D^2 + (1-y) * max(0, margin - D)^2
        # å…¶ä¸­ y=1 è¡¨ç¤ºç›¸ä¼¼ï¼ˆæ­£æ ·æœ¬å¯¹ï¼‰ï¼Œy=0 è¡¨ç¤ºä¸ç›¸ä¼¼ï¼ˆè´Ÿæ ·æœ¬å¯¹ï¼‰
        # D æ˜¯æ¬§æ°è·ç¦»

        # å¯¹äºæ­£æ ·æœ¬ (label == 1)ï¼Œå¸Œæœ›è·ç¦»å°ï¼ŒæŸå¤±æ˜¯è·ç¦»çš„å¹³æ–¹
        loss_positive = label * torch.pow(distance, 2)
        
        # å¯¹äºè´Ÿæ ·æœ¬ (label == 0)ï¼Œå¸Œæœ›è·ç¦»å¤§ï¼Œåªæœ‰å½“è·ç¦»å°äº margin æ—¶æ‰äº§ç”ŸæŸå¤±
        # Clamp ensures loss is 0 if distance is already larger than margin
        loss_negative = (1 - label) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)

        loss = torch.mean(loss_positive + loss_negative)
        return loss

# ====================== è®­ç»ƒå™¨ç±» (æ›´æ–°æ•°æ®é›†å‚æ•°ä¼ é€’) ======================
class NeuronTrainer:
    """ç¥ç»å…ƒè¿æ¥æ¨¡å‹è®­ç»ƒå™¨"""
    def __init__(self, config: dict):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.best_val_loss = float('inf')
        self.current_val_loss = float('inf')
        self.early_stop_counter = 0  # æ—©åœè®¡æ•°å™¨
        self.patience = config.get('patience', 10)  # æ—©åœå®¹å¿epochæ•°ï¼Œé»˜è®¤10
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        self.log_dir = os.path.join(config['log_dir'], f"train_{timestamp}")
        os.makedirs(self.log_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        with open(os.path.join(self.log_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)
        
        # åˆå§‹åŒ–TensorBoard
        self.writer = SummaryWriter(self.log_dir)
        
        # è®¾ç½®éšæœºç§å­
        self._set_seed(config['seed'])
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ¨¡å‹
        self._create_datasets()
        self.model = self._create_model()
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=config['lr'],
            weight_decay=config.get('weight_decay', 1e-5)
        )
        
        self.criterion = ContrastiveLoss(margin=config['margin'])
        
        print(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")

    def _set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        torch.manual_seed(seed)
        np.random.seed(seed)
        random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    
    def _create_model(self) -> nn.Module:
        """åˆ›å»ºå¹¶é…ç½®æ¨¡å‹"""
        # å®ä¾‹åŒ–EmbedNetUNet
        model = EmbedNetUNet(
            in_channels=1, 
            embed_dim=self.config['embed_dim'],
            base_channels=self.config.get('base_channels', 32), # å¯ä»¥åœ¨configä¸­æŒ‡å®šbase_channels
            use_se=self.config.get('use_se', True)
        ).to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print(f"æ¨¡å‹æ¶æ„:")
        print(model)
        print(f"å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        return model
    
    def _create_datasets(self):
        """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        # è®­ç»ƒæ•°æ®é›†
        self.train_dataset = NeuronConnectivityDataset(
            data_root=self.config['data_root'],
            img_dir=self.config['img_dir'],
            seg_dir=self.config['seg_dir'],
            split_file=self.config['train_split'],
            volume_size=self.config['volume_size'],
            augment=self.config.get('augment', False), # ä» config è·å– augment å‚æ•°
            max_translation=self.config.get('max_translation', 10) # ä» config è·å– max_translation å‚æ•°
        )
        
        # éªŒè¯æ•°æ®é›† (é€šå¸¸éªŒè¯é›†ä¸è¿›è¡Œæ•°æ®å¢å¼º)
        self.val_dataset = NeuronConnectivityDataset(
            data_root=self.config['data_root'],
            img_dir=self.config['img_dir'],
            seg_dir=self.config['seg_dir'],
            split_file=self.config['val_split'],
            volume_size=self.config['volume_size'],
            augment=False, # éªŒè¯é›†é€šå¸¸ä¸å¢å¼º
            max_translation=0 # éªŒè¯é›†ä¸å¢å¼ºï¼Œæ‰€ä»¥å¹³ç§»é‡ä¸º0
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(self.train_dataset)} ä¸ªä½“ç§¯")
        print(f"éªŒè¯é›†å¤§å°: {len(self.val_dataset)} ä¸ªä½“ç§¯")

    def _combine_inputs(self, img_batch: torch.Tensor, 
                         mask_query_batch: torch.Tensor,
                         mask_pos_batch: torch.Tensor,
                         mask_neg_batch: torch.Tensor) -> torch.Tensor:
        """
        ç»„åˆè¾“å…¥ï¼šå›¾åƒ + 3ä¸ªæ©ç ï¼Œä½œä¸ºEmbedNetçš„è¾“å…¥
        """
        # img_batch: [B, 1, D, H, W]
        # mask_X_batch: [B, 1, D, H, W]
        
        # æ‹¼æ¥å›¾åƒå’Œæ©ç  [B, 4, D, H, W]
        combined = torch.cat([img_batch, mask_query_batch, mask_pos_batch, mask_neg_batch], dim=1)
        
        return combined

    def _get_segment_embeddings(self, voxel_embeddings: torch.Tensor, 
                                 mask: torch.Tensor) -> torch.Tensor:
        """
        æ ¹æ®ä½“ç´ çº§åµŒå…¥å’Œå¯¹åº”åˆ†æ®µçš„æ©ç è®¡ç®—åˆ†æ®µå¹³å‡åµŒå…¥ã€‚
        
        voxel_embeddings: [B, embed_dim, D', H', W'] - EmbedNetçš„è¾“å‡º (ä¾‹å¦‚ 8x8x8)
        mask: [B, 1, D, H, W] - åŸå§‹å°ºå¯¸çš„äºŒå€¼æ©ç  (ä¾‹å¦‚ 128x128x128)
        
        è¿”å›: [B, embed_dim] - åˆ†æ®µçš„å¹³å‡åµŒå…¥
        """
        # è·å–ä½“ç´ åµŒå…¥çš„ç©ºé—´ç»´åº¦
        _, _, D_prime, H_prime, W_prime = voxel_embeddings.shape
        
        # å°†æ©ç ä¸‹é‡‡æ ·åˆ°ä¸ä½“ç´ åµŒå…¥ç›¸åŒçš„ç©ºé—´ç»´åº¦
        downsampled_mask = F.interpolate(
            mask, 
            size=(D_prime, H_prime, W_prime), 
            mode='nearest' 
        )
        
        # ç¡®ä¿æ©ç æ˜¯äºŒå€¼çš„
        downsampled_mask = (downsampled_mask > 0.5).float()

        # æ‰©å±•æ©ç ç»´åº¦ä»¥åŒ¹é…embed_dimï¼Œä»¥ä¾¿è¿›è¡Œå…ƒç´ ä¹˜æ³•
        # [B, 1, D', H', W'] -> [B, embed_dim, D', H', W']
        expanded_mask = downsampled_mask.expand_as(voxel_embeddings)
        
        # æå–åˆ†æ®µå†…çš„ä½“ç´ åµŒå…¥
        segment_voxel_features = voxel_embeddings * expanded_mask
        
        # è®¡ç®—æ¯ä¸ªåˆ†æ®µä¸­éé›¶ï¼ˆå³å±äºåˆ†æ®µï¼‰ä½“ç´ çš„æ•°é‡
        # [B, 1, D', H', W'] -> [B]
        num_voxels = torch.sum(downsampled_mask.squeeze(1), dim=(-3, -2, -1))
        
        # è®¡ç®—åˆ†æ®µå¹³å‡åµŒå…¥
        # å¯¹ç©ºé—´ç»´åº¦æ±‚å’Œ [B, embed_dim, D', H', W'] -> [B, embed_dim]
        sum_features = torch.sum(segment_voxel_features, dim=(-3, -2, -1))
        
        # é¿å…é™¤ä»¥é›¶ï¼Œå¯¹äºç©ºåˆ†æ®µï¼ˆnum_voxels=0ï¼‰ï¼ŒåµŒå…¥å¯ä»¥è®¾ä¸ºé›¶å‘é‡
        avg_embeddings = torch.where(
            num_voxels.unsqueeze(1) > 0, 
            sum_features / num_voxels.unsqueeze(1),
            torch.zeros_like(sum_features) 
        )
        
        return avg_embeddings

    def train_epoch(self, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        progress_bar = tqdm(self.train_loader, desc=f"è®­ç»ƒ Epoch {epoch}")
        
        # (img_tensor, mask_query_tensor, mask_pos_tensor, mask_neg_tensor, label_pos_pair, label_neg_pair)
        for batch_idx, (imgs, mask_q, mask_p, mask_n, label_pp, label_np) in enumerate(progress_bar):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            imgs, mask_q, mask_p, mask_n = imgs.to(self.device), mask_q.to(self.device), \
                                             mask_p.to(self.device), mask_n.to(self.device)
            label_pp, label_np = label_pp.to(self.device), label_np.to(self.device)
            
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ï¼Œè·å–ä½“ç´ çº§åµŒå…¥ç‰¹å¾å›¾
            # voxel_embeddings: [B, embed_dim, D', H', W']
            voxel_embeddings = self.model(imgs)
            
            # æ ¹æ®è®ºæ–‡ï¼Œä»ä½“ç´ çº§åµŒå…¥ä¸­è®¡ç®—åˆ†æ®µçš„å¹³å‡åµŒå…¥
            # è¿™é‡Œè®¡ç®—çš„æ˜¯ Query Segment (e_Q), Positive Segment (e_P), Negative Segment (e_N) çš„å¹³å‡åµŒå…¥
            # mask_q, mask_p, mask_n æ˜¯åŸå§‹å°ºå¯¸çš„æ©ç 
            e_query = self._get_segment_embeddings(voxel_embeddings, mask_q)
            e_pos = self._get_segment_embeddings(voxel_embeddings, mask_p)
            e_neg = self._get_segment_embeddings(voxel_embeddings, mask_n)
            # è®¡ç®—å¯¹æ¯”æŸå¤±
            # 1. ä¿ƒä½¿ (e_query, e_pos) ç›¸ä¼¼ (label_pp = 1.0)
            loss_qp = self.criterion(e_query, e_pos, label_pp) 
            
            # 2. ä¿ƒä½¿ (e_query, e_neg) ä¸ç›¸ä¼¼ (label_np = 0.0)
            loss_qn = self.criterion(e_query, e_neg, label_np) 
            
            # æ€»æŸå¤±
            loss = loss_qp + loss_qn
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # è®°å½•æŸå¤±
            total_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            avg_loss = total_loss / (batch_idx + 1)
            progress_bar.set_postfix(loss=f"{loss.item():.16f}", avg_loss=f"{avg_loss:.16f}", loss_qp=f"{loss_qp.item():.16f}", loss_qn=f"{loss_qn.item():.16f}")
            
            # è®°å½•TensorBoard
            global_step = epoch * len(self.train_loader) + batch_idx
            self.writer.add_scalar('Loss/train_batch', loss.item(), global_step)
        
        return total_loss / len(self.train_loader)
    
    def validate(self, epoch: int) -> float:
        """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°"""
        self.model.eval()
        total_loss = 0.0
        progress_bar = tqdm(self.val_loader, desc=f"éªŒè¯ Epoch {epoch}")
        
        with torch.no_grad():
            # (img_tensor, mask_query_tensor, mask_pos_tensor, mask_neg_tensor, label_pos_pair, label_neg_pair)
            for batch_idx, (imgs, mask_q, mask_p, mask_n, label_pp, label_np) in enumerate(progress_bar):
                imgs, mask_q, mask_p, mask_n = imgs.to(self.device), mask_q.to(self.device), \
                                                 mask_p.to(self.device), mask_n.to(self.device)
                label_pp, label_np = label_pp.to(self.device), label_np.to(self.device)
                
                
                # å‰å‘ä¼ æ’­
                voxel_embeddings = self.model(imgs)
                
                # è®¡ç®—åˆ†æ®µå¹³å‡åµŒå…¥
                e_query = self._get_segment_embeddings(voxel_embeddings, mask_q)
                e_pos = self._get_segment_embeddings(voxel_embeddings, mask_p)
                e_neg = self._get_segment_embeddings(voxel_embeddings, mask_n)
                
                # è®¡ç®—æŸå¤±
                loss_qp = self.criterion(e_query, e_pos, label_pp)
                loss_qn = self.criterion(e_query, e_neg, label_np)
                loss = loss_qp + loss_qn
                total_loss += loss.item()
                
                # æ›´æ–°è¿›åº¦æ¡
                avg_loss = total_loss / (batch_idx + 1)
                progress_bar.set_postfix(avg_loss=f"{avg_loss:.4f}")
        
        avg_loss = total_loss / len(self.val_loader)
        self.writer.add_scalar('Loss/val', avg_loss, epoch)
        
        return avg_loss
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        state = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_loss': self.current_val_loss,
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(self.log_dir, 'checkpoint_latest.pth')
        torch.save(state, latest_path)
        
        # ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
        if is_best:
            best_path = os.path.join(self.log_dir, 'checkpoint_best.pth')
            torch.save(state, best_path)
            print(f"ğŸ”¥ ä¿å­˜æœ€ä½³æ¨¡å‹: éªŒè¯æŸå¤± {self.best_val_loss:.4f}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\n" + "="*60)
        print(f"å¼€å§‹è®­ç»ƒ {self.config['epochs']} ä¸ªå‘¨æœŸ")
        print("="*60)
        print(f"æ—¥å¿—ç›®å½•: {self.log_dir}")
        print(f"è¾“å…¥å°ºå¯¸: {self.config['volume_size']}")
        
        for epoch in range(1, self.config['epochs'] + 1):
            start_time = time.time()
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            self.current_val_loss = self.validate(epoch)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = self.current_val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = self.current_val_loss
                self.early_stop_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
            else:
                self.early_stop_counter += 1
                print(f"   æ—©åœè®¡æ•°: {self.early_stop_counter}/{self.patience}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch, is_best)
            
            # æ‰“å°epochæ‘˜è¦
            epoch_time = time.time() - start_time
            print(f"\nEpoch {epoch}/{self.config['epochs']} æ‘˜è¦:")
            print(f"   è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {self.current_val_loss:.4f}")
            print(f"   æ—¶é—´: {epoch_time:.1f}ç§’ | å­¦ä¹ ç‡: {self.config['lr']:.6f}")
            
            if is_best:
                print(f"   ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            
            # æ˜¾å¼é‡Šæ”¾å†…å­˜
            torch.cuda.empty_cache()
            gc.collect()
            
            # æ—©åœåˆ¤æ–­
            if self.early_stop_counter >= self.patience:
                print(f"\n>>> æ—©åœè§¦å‘: éªŒè¯æŸå¤±è¿ç»­ {self.patience} ä¸ªepochæœªæå‡ï¼Œæå‰ç»ˆæ­¢è®­ç»ƒã€‚")
                break
        
        print("\n" + "="*60)
        print(f"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print("="*60)
        self.writer.close()

# ====================== ä¸»å‡½æ•° ======================
if __name__ == "__main__":
    # è®­ç»ƒé…ç½®
    config = {
        # æ•°æ®é…ç½®
        'data_root': '/nvme2/mingzhi/NucCorr/correct_baseline/contrastive_learning/data',
        'img_dir': 'img',
        'seg_dir': 'seg',
        'train_split': 'train.txt',
        'val_split': 'val.txt',
        'volume_size': (128, 128, 128),  # å›ºå®šä½“ç§¯å°ºå¯¸
        'augment': True,                 # å¯ç”¨æ•°æ®å¢å¼º
        'max_translation': 10,           # æœ€å¤§å¹³ç§»åƒç´ å€¼ (ä¾‹å¦‚ï¼Œåœ¨D,H,Wæ–¹å‘ä¸Šå¯å¹³ç§»-10åˆ°+10åƒç´ )
        
        # æ¨¡å‹é…ç½®
        'embed_dim': 16, # æ¯ä¸ªä½“ç´ çš„åµŒå…¥ç»´åº¦
        'base_channels': 32, # U-Netåˆå§‹é€šé“æ•°
        'use_se': True, # æ˜¯å¦ä½¿ç”¨SEå±‚

        # è®­ç»ƒé…ç½®
        'batch_size': 8,
        'epochs': 50,
        'lr': 0.001,
        'num_workers': 4,
        'seed': 42,
        
        # æŸå¤±é…ç½®
        'margin': 10,
        
        # æ—¥å¿—é…ç½®
        'log_dir': './logs',
        'patience': 5,  # æ–°å¢ï¼šæ—©åœå®¹å¿epochæ•°
    }
    
    # åˆ›å»ºå¹¶è¿è¡Œè®­ç»ƒå™¨
    trainer = NeuronTrainer(config)
    trainer.train()